# ðŸ› ï¸ Backend Implementation Guide

**Building a Production-Ready REST API for Road Quality Monitoring**

---

## ðŸ“š Stack & Environment Setup

### **Docker Environment**

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# System dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    gdal-bin \
    libgdal-dev \
    && rm -rf /var/lib/apt/lists/*

# Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```text
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.4.2
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
redis==5.0.1
kafka-python==2.0.2
pytorchvision==0.16.0
ultralytics==8.0.226
opencv-python==4.8.1.78
geopandas==0.13.2
shapely==2.0.2
pyproj==3.6.1
pyjwt==2.8.1
python-dotenv==1.0.0
prometheus-client==0.18.0
structlog==23.2.1
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/road_db
      - REDIS_URL=redis://redis:6379
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - JWT_SECRET=your_secret_key_here_min_32_chars
      - LOG_LEVEL=INFO
    depends_on:
      - postgres
      - redis
      - kafka
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  postgres:
    image: postgis/postgis:16-3.4
    environment:
      POSTGRES_DB: road_db
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_db.sql:/docker-entrypoint-initdb.d/init.sql

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 336  # 2 weeks
    depends_on:
      - zookeeper

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    depends_on:
      - prometheus

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
```

---

## ðŸ—„ï¸ Database Schema

```sql
-- init_db.sql
CREATE EXTENSION IF NOT EXISTS postgis;
CREATE EXTENSION IF NOT EXISTS uuid-ossp;

-- Devices table
CREATE TABLE devices (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    device_id VARCHAR(50) UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    organization_id VARCHAR(50) NOT NULL,
    device_type VARCHAR(50) NOT NULL,  -- 'dashcam', 'smartphone', 'embedded'
    status VARCHAR(20) DEFAULT 'online',  -- 'online', 'offline', 'inactive'
    last_heartbeat TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    camera_specs JSONB,                    -- Resolution, FPS, sensor size
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    INDEX idx_organization_id (organization_id),
    INDEX idx_device_id (device_id)
);

-- Detections table
CREATE TABLE detections (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    event_id UUID NOT NULL UNIQUE,
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    gps_location GEOGRAPHY(POINT, 4326) NOT NULL,  -- PostGIS geometry
    gps_accuracy FLOAT,
    gps_altitude FLOAT,
    gps_heading FLOAT,
    gps_speed FLOAT,
    
    detections JSONB NOT NULL,                     -- Array of detection objects
    detection_count INT,
    
    status VARCHAR(20) DEFAULT 'processing',       -- 'processing', 'completed', 'failed'
    processing_time_ms INT,
    
    road_type VARCHAR(50),                         -- 'highway', 'urban', 'rural'
    weather VARCHAR(50),
    
    image_path VARCHAR(255),                       -- S3 path to stored image
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    FOREIGN KEY (device_id) REFERENCES devices(device_id),
    INDEX idx_device_id (device_id),
    INDEX idx_timestamp (timestamp),
    INDEX idx_gps_location (gps_location),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
);

-- Damage classifications table
CREATE TABLE damage_classif (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    detection_id UUID NOT NULL REFERENCES detections(id),
    
    class VARCHAR(50) NOT NULL,            -- 'pothole', 'crack', 'wear'
    confidence FLOAT NOT NULL,
    bbox JSONB NOT NULL,                   -- [x1, y1, x2, y2]
    
    severity VARCHAR(20) NOT NULL,         -- 'HIGH', 'MEDIUM', 'LOW'
    severity_score FLOAT,
    repair_priority INT,
    estimated_size_m2 FLOAT,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    FOREIGN KEY (detection_id) REFERENCES detections(id),
    INDEX idx_detection_id (detection_id),
    INDEX idx_class (class),
    INDEX idx_severity (severity)
);

-- API keys table
CREATE TABLE api_keys (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    device_id VARCHAR(50) NOT NULL REFERENCES devices(device_id),
    key_hash VARCHAR(255) NOT NULL UNIQUE,
    key_prefix VARCHAR(10),                -- For display only (dev_xxxx...)
    is_active BOOLEAN DEFAULT TRUE,
    last_used TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    expires_at TIMESTAMP WITH TIME ZONE,
    
    INDEX idx_device_id (device_id),
    INDEX idx_key_hash (key_hash)
);

-- Statistics/aggregates (materialized)
CREATE TABLE detection_stats (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    date DATE NOT NULL,
    hour INT,                              -- 0-23, NULL for day-level
    device_id VARCHAR(50),
    class VARCHAR(50),
    severity VARCHAR(20),
    
    count INT NOT NULL,
    avg_confidence FLOAT,
    
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE (date, hour, device_id, class, severity),
    INDEX idx_date (date),
    INDEX idx_device_id (device_id),
    INDEX idx_class (class)
);

-- Create indexes for geospatial queries
CREATE INDEX idx_detections_gps ON detections USING GIST (gps_location);

-- Refresh materialized view every hour
-- (Scheduled via pg_cron extension)
CREATE MATERIALIZED VIEW detection_hotspots AS
SELECT 
    ST_ClusterKMeans(gps_location, 20) OVER () as cluster_id,
    ST_Centroid(ST_Collect(gps_location)) as hotspot_location,
    COUNT(*) as detection_count,
    AVG(CAST((detections->0->>'confidence')::FLOAT AS FLOAT)) as avg_confidence,
    MAX(timestamp) as last_detection
FROM detections
WHERE timestamp > NOW() - INTERVAL 30 DAYS
GROUP BY cluster_id;

CREATE INDEX idx_hotspots_location ON detection_hotspots USING GIST (hotspot_location);
```

---

## ðŸ’» FastAPI Implementation

### **Main Application**

```python
# main.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
import os
from contextlib import asynccontextmanager

import structlog
from prometheus_client import Counter, Histogram

# Initialize logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)
logger = structlog.get_logger()

# Metrics
detection_submissions = Counter(
    'road_detection_submissions_total',
    'Total detection submissions',
    ['status']
)
api_request_latency = Histogram(
    'road_api_latency_seconds',
    'API request latency'
)

# Lifespan context for startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting Road Quality Monitor API")
    await setup_database()
    await connect_redis()
    await connect_kafka()
    
    yield
    
    # Shutdown
    logger.info("Shutting down")
    await close_database()
    await close_redis()
    await close_kafka()

app = FastAPI(
    title="Road Quality Monitor API",
    version="1.0.0",
    description="Real-time road damage detection and analytics",
    lifespan=lifespan
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Health check
@app.get("/health")
async def health():
    return {
        "status": "ok",
        "version": "1.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }

# Include routers
from routers import detections, analytics, devices, reports
app.include_router(detections.router, prefix="/api/v1", tags=["Detections"])
app.include_router(analytics.router, prefix="/api/v1", tags=["Analytics"])
app.include_router(devices.router, prefix="/api/v1", tags=["Devices"])
app.include_router(reports.router, prefix="/api/v1", tags=["Reports"])

# Exception handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    logger.warning(f"HTTP exception: {exc.status_code} - {exc.detail}",
                  path=request.url.path, status_code=exc.status_code)
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.utcnow().isoformat()
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### **Detections Router**

```python
# routers/detections.py
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, validator
from datetime import datetime
from uuid import uuid4
import json

from database import get_db
from auth import verify_token, AuthToken
from kafka_client import kafka_producer
from models import Detection, Device
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

# â”€â”€ Request Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GPSData(BaseModel):
    lat: float
    lon: float
    accuracy: float
    altitude: float = None
    heading: float = None
    speed: float = None
    
    @validator('lat')
    def validate_lat(cls, v):
        if not -90 <= v <= 90:
            raise ValueError('Latitude out of range')
        return v
    
    @validator('lon')
    def validate_lon(cls, v):
        if not -180 <= v <= 180:
            raise ValueError('Longitude out of range')
        return v

class BoundingBox(BaseModel):
    bbox: list[int]  # [x1, y1, x2, y2]
    class_: str = None
    confidence: float
    class_id: int
    
    @validator('confidence')
    def validate_confidence(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('Confidence must be 0-1')
        return v

class DetectionSubmission(BaseModel):
    device_id: str
    timestamp: datetime
    gps: GPSData
    detections: list[BoundingBox]
    metadata: dict = None
    frame: dict = None
    image_base64: str = None

class DetectionResponse(BaseModel):
    event_id: str
    status: str
    processing_eta_seconds: int
    created_at: datetime

# â”€â”€ API Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@router.post("/detections", response_model=DetectionResponse, status_code=202)
async def submit_detection(
    payload: DetectionSubmission,
    auth: AuthToken = Depends(verify_token),
    db = Depends(get_db)
):
    """Submit road damage detections from vehicle/camera."""
    
    event_id = str(uuid4())
    
    try:
        # Validate GPS accuracy
        if payload.gps.accuracy > 100:
            raise HTTPException(
                status_code=400,
                detail="GPS accuracy too poor (>100m). Minimum 5m required"
            )
        
        # Verify device exists
        device = db.query(Device).filter_by(device_id=payload.device_id).first()
        if not device:
            raise HTTPException(
                status_code=404,
                detail=f"Device {payload.device_id} not found"
            )
        
        # Check for duplicates (same location Â± 10m, same class, < 30 sec)
        recent_detection = check_duplicate(
            db, payload.gps, payload.detections, device_id=payload.device_id
        )
        if recent_detection:
            raise HTTPException(
                status_code=409,
                detail=f"Similar detection already recorded",
                headers={"X-Existing-Event-ID": str(recent_detection.event_id)}
            )
        
        # Enqueue to Kafka for async processing
        message = {
            "event_id": event_id,
            "device_id": payload.device_id,
            "timestamp": payload.timestamp.isoformat(),
            "payload": payload.dict(exclude={"image_base64"}),
            "image_base64": payload.image_base64
        }
        
        await kafka_producer.send_and_wait(
            "events/road-damage",
            json.dumps(message).encode('utf-8')
        )
        
        # Log submission
        logger.info(f"Detection submitted", event_id=event_id, device_id=payload.device_id,
                   detection_count=len(payload.detections))
        
        detection_submissions.labels(status='submitted').inc()
        
        return DetectionResponse(
            event_id=event_id,
            status="queued",
            processing_eta_seconds=5,
            created_at=datetime.utcnow()
        )
    
    except Exception as e:
        logger.error(f"Detection submission failed", event_id=event_id, error=str(e))
        detection_submissions.labels(status='failed').inc()
        raise HTTPException(
            status_code=500,
            detail="Internal server error during submission"
        )

@router.get("/detections/{event_id}")
async def get_detection_status(
    event_id: str,
    auth: AuthToken = Depends(verify_token),
    db = Depends(get_db)
):
    """Get status and results of a detection."""
    
    detection = db.query(Detection).filter_by(event_id=event_id).first()
    if not detection:
        raise HTTPException(status_code=404, detail="Detection not found")
    
    result = {
        "event_id": detection.event_id,
        "status": detection.status,
        "device_id": detection.device_id,
        "created_at": detection.created_at,
        "processed_at": detection.updated_at if detection.status == "completed" else None,
        "processing_time_ms": detection.processing_time_ms,
    }
    
    if detection.status == "completed":
        # Query damage classifications
        damages = db.query(DamageClassif).filter_by(detection_id=detection.id).all()
        result["detections"] = [
            {
                "class": d.class_,
                "confidence": d.confidence,
                "bbox": d.bbox,
                "severity": d.severity,
                "severity_score": d.severity_score,
                "repair_priority": d.repair_priority,
                "estimated_size_m2": d.estimated_size_m2
            }
            for d in damages
        ]
        
        # Geolocation
        from geopy.geocoders import Nominatim
        geolocator = Nominatim(user_agent="road-monitor")
        location = geolocator.reverse(f"{detection.gps_location.y}, {detection.gps_location.x}")
        
        result["location"] = {
            "lat": detection.gps_location.y,
            "lon": detection.gps_location.x,
            "address": location.address if location else "Unknown"
        }
    
    return result

def check_duplicate(db, gps, detections, device_id, time_window_sec=30, distance_m=10):
    """Check if similar detection already recorded."""
    
    from sqlalchemy.sql import func
    from geoalchemy2.functions import ST_DWithin
    from datetime import timedelta
    
    point = f"SRID=4326;POINT({gps.lon} {gps.lat})"
    
    recent = db.query(Detection).filter(
        Detection.device_id == device_id,
        Detection.timestamp > datetime.utcnow() - timedelta(seconds=time_window_sec),
        ST_DWithin(Detection.gps_location, point, distance_m / 111000)  # meters to degrees
    ).first()
    
    return recent
```

### **Inference Worker (Kafka Consumer)**

```python
# workers/inference_worker.py
import asyncio
import json
import logging
from kafka import KafkaConsumer
from ultralytics import YOLO
import cv2
import base64
import numpy as np
from datetime import datetime
from database import SessionLocal

logger = logging.getLogger(__name__)

class InferenceWorker:
    def __init__(self):
        self.model = YOLO("models/best.pt")
        self.consumer = KafkaConsumer(
            'events/road-damage',
            bootstrap_servers=['kafka:9092'],
            group_id='inference-workers',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        self.db = SessionLocal()
    
    async def process_message(self, message):
        """Process detection from Kafka queue."""
        
        event_id = message['event_id']
        payload = message['payload']
        image_base64 = message.get('image_base64')
        
        try:
            logger.info(f"Processing event {event_id}")
            start_time = datetime.now()
            
            # Decode image
            if image_base64:
                img_data = base64.b64decode(image_base64)
                nparr = np.frombuffer(img_data, np.uint8)
                frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            else:
                frame = None
            
            # Run YOLOv8 inference if image provided
            results = None
            if frame is not None:
                results = self.model(frame, conf=0.4, iou=0.45)
            
            # Severity classification
            from severity_classifier import SeverityClassifier
            severity_clf = SeverityClassifier()
            
            damages = []
            for detection in payload['detections']:
                severity_result = severity_clf.classify(
                    detection['bbox'],
                    (payload['frame']['height'], payload['frame']['width']),
                    detection['class_'],
                    detection['confidence']
                )
                
                damages.append({
                    'bbox': detection['bbox'],
                    'class': detection['class_'],
                    'confidence': detection['confidence'],
                    'severity': severity_result.level,
                    'severity_score': severity_result.score,
                    'repair_priority': severity_result.repair_priority,
                    'estimated_size_m2': severity_result.estimated_size_m2
                })
            
            # Store in database
            from models import Detection, DamageClassif
            
            detection = Detection(
                event_id=event_id,
                device_id=payload['device_id'],
                timestamp=datetime.fromisoformat(payload['timestamp']),
                gps_location=f"SRID=4326;POINT({payload['gps']['lon']} {payload['gps']['lat']})",
                gps_accuracy=payload['gps']['accuracy'],
                detections=damages,
                status='completed',
                processing_time_ms=int((datetime.now() - start_time).total_seconds() * 1000),
                road_type=payload['metadata'].get('road_type'),
                weather=payload['metadata'].get('weather')
            )
            
            self.db.add(detection)
            
            for damage in damages:
                dam_classif = DamageClassif(
                    detection=detection,
                    class_=damage['class'],
                    confidence=damage['confidence'],
                    bbox=damage['bbox'],
                    severity=damage['severity'],
                    severity_score=damage['severity_score'],
                    repair_priority=damage['repair_priority'],
                    estimated_size_m2=damage['estimated_size_m2']
                )
                self.db.add(dam_classif)
            
            self.db.commit()
            
            logger.info(f"Event {event_id} processed in {detection.processing_time_ms}ms")
        
        except Exception as e:
            logger.error(f"Error processing event {event_id}: {str(e)}")
            self.db.rollback()
    
    async def run(self):
        """Main worker loop."""
        logger.info("Inference worker started")
        
        for message in self.consumer:
            try:
                await self.process_message(message.value)
            except Exception as e:
                logger.error(f"Worker error: {str(e)}")

# Entry point
if __name__ == "__main__":
    worker = InferenceWorker()
    asyncio.run(worker.run())
```

---

## ðŸ” Authentication

```python
# auth.py
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthCredentials
import jwt
from datetime import datetime, timedelta
from typing import Optional

JWT_SECRET = os.getenv("JWT_SECRET", "your_secret_key_here_min_32_chars")
JWT_ALGORITHM = "HS256"
JWT_EXPIRATION_HOURS = 24

security = HTTPBearer()

class AuthToken:
    def __init__(self, device_id: str, org_id: str, scopes: list[str]):
        self.device_id = device_id
        self.org_id = org_id
        self.scopes = scopes

def generate_token(device_id: str, org_id: str, scopes: list[str] = None,
                  expires_in_hours: int = JWT_EXPIRATION_HOURS) -> str:
    """Generate JWT token for device."""
    
    if scopes is None:
        scopes = ["read:detections", "write:detections", "read:analytics"]
    
    payload = {
        "device_id": device_id,
        "org_id": org_id,
        "scopes": scopes,
        "iat": datetime.utcnow(),
        "exp": datetime.utcnow() + timedelta(hours=expires_in_hours)
    }
    
    token = jwt.encode(payload, JWT_SECRET, algorithm=JWT_ALGORITHM)
    return token

async def verify_token(credentials: HTTPAuthCredentials = Depends(security)) -> AuthToken:
    """Verify JWT token from Authorization header."""
    
    token = credentials.credentials
    
    try:
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGORITHM])
        
        device_id = payload.get("device_id")
        org_id = payload.get("org_id")
        scopes = payload.get("scopes", [])
        
        if not device_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
        
        return AuthToken(device_id=device_id, org_id=org_id, scopes=scopes)
    
    except jwt.ExpiredSignatureError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token expired"
        )
    except jwt.InvalidTokenError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )
```

---

## ðŸ“Š Analytics Endpoints

```python
# routers/analytics.py
from fastapi import APIRouter, Depends
from database import get_db
from sqlalchemy import func, text
from geoalchemy2.functions import ST_AsGeoJSON
from datetime import datetime, timedelta

router = APIRouter()

@router.get("/analytics/summary")
async def get_summary(days: int = 1, db = Depends(get_db)):
    """Get system statistics and damage summary."""
    
    from models import Detection, DamageClassif
    
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # Total detections
    total = db.query(func.count(Detection.id)).filter(
        Detection.created_at >= start_date
    ).scalar()
    
    # By severity
    severity_breakdown = db.query(
        DamageClassif.severity,
        func.count(DamageClassif.id).label('count')
    ).join(Detection).filter(
        Detection.created_at >= start_date
    ).group_by(DamageClassif.severity).all()
    
    # By class
    class_breakdown = db.query(
        DamageClassif.class_,
        func.count(DamageClassif.id).label('count')
    ).join(Detection).filter(
        Detection.created_at >= start_date
    ).group_by(DamageClassif.class_).all()
    
    # Trend (hourly)
    trend = db.query(
        func.date_trunc('hour', Detection.created_at).label('hour'),
        func.count(Detection.id).label('count')
    ).filter(
        Detection.created_at >= start_date
    ).group_by(func.date_trunc('hour', Detection.created_at)).all()
    
    # Top hotspots
    hotspots = db.query(
        text("ST_AsLatLngJSON(hotspot_location)"),
        text("detection_count")
    ).from_statement(
        text("SELECT * FROM detection_hotspots ORDER BY detection_count DESC LIMIT 10")
    ).all()
    
    return {
        "timestamp": datetime.utcnow(),
        "period_days": days,
        "summary": {"total": total},
        "severity_breakdown": [
            {"severity": s[0], "count": s[1]} for s in severity_breakdown
        ],
        "class_breakdown": [
            {"class": c[0], "count": c[1]} for c in class_breakdown
        ],
        "trend": [
            {"hour": str(t[0]), "count": t[1]} for t in trend
        ],
        "hotspots": hotspots
    }

@router.get("/detections/location")
async def query_by_location(
    lat_min: float, lat_max: float,
    lon_min: float, lon_max: float,
    severity: str = None,
    class_: str = None,
    days: int = 7,
    limit: int = 100,
    offset: int = 0,
    db = Depends(get_db)
):
    """Query detections within geographic bounding box."""
    
    from models import Detection, DamageClassif
    from geoalchemy2.functions import ST_Intersects, ST_MakeEnvelope
    
    bbox = ST_MakeEnvelope(lon_min, lat_min, lon_max, lat_max, 4326)
    
    query = db.query(Detection, DamageClassif).join(
        DamageClassif, Detection.id == DamageClassif.detection_id
    ).filter(
        ST_Intersects(Detection.gps_location, bbox),
        Detection.created_at >= datetime.utcnow() - timedelta(days=days)
    )
    
    if severity:
        severities = [s.strip() for s in severity.split(',')]
        query = query.filter(DamageClassif.severity.in_(severities))
    
    if class_:
        classes = [c.strip() for c in class_.split(',')]
        query = query.filter(DamageClassif.class_.in_(classes))
    
    total = query.count()
    detections = query.offset(offset).limit(limit).all()
    
    return {
        "query": {
            "bounds": {"lat_min": lat_min, "lat_max": lat_max, "lon_min": lon_min, "lon_max": lon_max},
            "time_range": f"last {days} days"
        },
        "results": {
            "total": total,
            "returned": len(detections),
            "detections": [
                {
                    "detection_id": str(det.event_id),
                    "location": {
                        "lat": det.gps_location.y,
                        "lon": det.gps_location.x
                    },
                    "class": dam.class_,
                    "severity": dam.severity,
                    "confidence": dam.confidence,
                    "timestamp": det.timestamp.isoformat()
                }
                for det, dam in detections
            ]
        },
        "pagination": {
            "has_next": offset + limit < total,
            "next_offset": offset + limit if offset + limit < total else None
        }
    }
```

---

## ðŸš€ Deployment Instructions

```bash
# 1. Build and run with Docker Compose
docker-compose up -d

# 2. Apply database migrations
docker-compose exec api alembic upgrade head

# 3. Create superuser/admin
docker-compose exec api python -c "
from auth import generate_token
token = generate_token('ADMIN_001', 'ORG_ADMIN')
print(f'Admin token: {token}')
"

# 4. Test API
curl -X GET http://localhost:8000/health

# 5. View logs
docker-compose logs -f api

# 6. Monitor metrics
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000
```

---

This implementation provides a **production-ready backend** with comprehensive error handling, authentication, database integration, and real-time processing capabilities.

